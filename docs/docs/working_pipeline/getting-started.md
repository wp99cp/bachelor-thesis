# Deep Learning for Accurate Snow and Cloud Segmentation in Alpine Landscapes

This is the documentation for my bachelor thesis. It contains all the information needed to reproduce the results, train
the model, and run inference on new images.

To get started, follow the steps described below:

1) [Installation and Setup](#installation-and-setup)
2) [Folder Structure and Data Organization](#folder-structure-and-data-organization)
3) [Setting Environments Variables](#setting-environments-variables)

## Installation and Setup

Before you can get started, you need to install the necessary dependencies:

::: danger
This is not yet documented. See [Python and Conda](/docs/nice_to_know/python_and_conda) for the meanwhile.
:::

## Folder Structure and Data Organization

The data organization for a project of that size is crucial. Thus, we avoid the usage of absolute paths wherever
possible and use environment variables instead. This allows us to easily switch between different machines and
environments with minimal effort. Thus make sure to export the environment variables before starting,
see [Environment Variables](#setting-environments-variables).

Nevertheless, we propose a folder structure that is used throughout the project.

```text
- project_root_directory (cloned repo)
  |- data
     |- sentinel2
        |- raw_data
           |- T32TNS
              |- S2A_MSIL1C_*.zip (raw data zip, downloaded form sentinel hub)
              |- S2B_MSIL1C_*.zip (raw data zip, downloaded form sentinel hub)
              |- ...
           |- T13VEH
           |- ... 
        |- exoLabs
           |- T32TNS
           |- ...
        |- masks
           |- T32TNS
              |- autogenerated_masks
              |- cleaned_masks
              |- ...
     |- landsat8
        |- ...
     |- auxiliary_data (satellite independent data)
        |- T32TNS
        |- ...
  |- results
     |- training_checkpoints
        |- {{training_id}} (commit hash with random suffix)
        |- ...
     |- inference_results
        |- sentinel_2
           |- T32TNS
              |- S2A_MSIL1C_*
                 |- {{training_id}}
              |- ...
        |- landsat_8
     |- ...
  |- tmp
  |- ...
```

### Auxiliary Data

The auxiliary data folder must contain the following files:

```bash
$ data/auxiliary_data/07VEH$ ll
07VEH_30m_DEM_AW3D30.tif
07VEH_30m_Glacier_RGIv6.tif
07VEH_30m_JRC_surfaceWater.tif
07VEH_30m_treeCanopyCover.tif
```

## Structure of the Source Code

All the source code used for training and evaluation of the model can be found in the `/src` directory. The `utilities`
folder contains the launch scripts and some helper python functions used to visualize the data and the results. The
`utilities/ansible`, `utilities/euler`  and `utilities/deploy_script` folder contains the ansible scripts used to manage
the data on the ETH cluster.

```text


## Setting Environments Variables

The segmentation pipelines is designed to work on different devices with potentially different file structures, thus it
is unavoidable to set some environment variables. The following sections describe how to set the environment variables.

::: details Working on a Workstation

Here all data associated with the project is stored within the `/projects/bachelor-thesis` folder.

The following commands are saved in the `~/.bashrc` file, thus you don't need to execute them manually.
However, for the sake of completeness, they are listed here:

```bash
# project base directory, this directory contains
# all the source code and data
export BASE_DIR=/scratch2/pucyril/bachelor-thesis

export TMP_DIR=$BASE_DIR/tmp

# paths to the data
export DATA_DIR=$BASE_DIR/data
export DATA_SENTINEL2=$BASE_DIR/data/sentinel2
export DATA_LANDSAT8=$BASE_DIR/data/landsat8
export AUXILIARY_DATA_DIR=$BASE_DIR/data/auxiliary_data

# here the results are stored (e.g. model checkpoints, tensorboard logs, etc.)
export RESULTS_DIR=$BASE_DIR/results

# add additional python paths
export PYTHONPATH="${PYTHONPATH}:/scratch2/pucyril/python"

export USE_CONDA_ENVIRONMENT=1
export CONDA_ENVIRONMENT_NAME=bachelor_thesis
```

:::

::: details Working with Euler (ETH Cluster)
Here all data management is handled by the ansible scripts. Details can be
found [in the section about Euler](/docs/nice_to_know/euler.html).
:::

